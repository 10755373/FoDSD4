{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd0d657e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebb44425",
   "metadata": {},
   "source": [
    "# UN Example Links\n",
    "\n",
    "- [Baturo 2017 paper](https://arxiv.org/pdf/1708.05873.pdf)\n",
    "- [Medium.com - UN EDA](https://medium.com/@anushkocharyan/nlp-analysis-of-50-years-of-united-nations-general-debate-speeches-61dc3bed3c11)\n",
    "- [Github - Medium.com UN EDA](https://github.com/anushkocharyan/NLP_Analysis_on_UN_speeches)\n",
    "- [Kaggle - UN India EDA](https://www.kaggle.com/someadityamandal/analysis-of-india-at-un-debates)\n",
    "- [Github - Random Guy](https://github.com/nicolasdz/UNGDC/blob/main/UNGDC%204%2C0.ipynb)\n",
    "\n",
    "# NLP Technical Links\n",
    "- [Medium.com - Text preprocessing](https://medium.com/@datamonsters/text-preprocessing-in-python-steps-tools-and-examples-bf025f872908)\n",
    "- [Stemming-Lemmatization](https://www.guru99.com/stemming-lemmatization-python-nltk.html)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa76598c",
   "metadata": {},
   "source": [
    "# Import Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1a4a8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import country codes\n",
    "df_code = pd.read_csv('UNSD — Methodology.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f12c5cc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import speeches\n",
    "\n",
    "sessions = np.arange(25, 76)\n",
    "data=[]\n",
    "\n",
    "for session in sessions:\n",
    "    directory = \"./TXT/Session \"+str(session)+\" - \"+str(1945+session)\n",
    "    \n",
    "    for filename in os.listdir(directory):\n",
    "        f = open(os.path.join(directory, filename))\n",
    "        \n",
    "        if filename[0]==\".\": #ignore hidden files\n",
    "            continue\n",
    "        splt = filename.split(\"_\")\n",
    "        data.append([session, 1945+session, splt[0], f.read()])\n",
    "\n",
    "df_speech = pd.DataFrame(data, columns=['Session','Year','ISO-alpha3 Code','Speech'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "50273be7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import happiness index\n",
    "\n",
    "df_happiness = pd.read_excel('DataPanelWHR2021C2.xls')\n",
    "df_happiness.rename(columns={'year':'Year'}, inplace=True) # for merge later\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93bb5d81",
   "metadata": {},
   "source": [
    "# Inspect Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4ac69ec",
   "metadata": {},
   "source": [
    "df_codes and df_speech merged on **ISO-alpha3 Code**\n",
    "\n",
    "**What alpha3 codes are found in df_speech but NOT df_code?**\n",
    "\n",
    "- EU = European Union (since 2011)\n",
    "- DDR = German Democratic Republic --> now Germany\n",
    "- POR = incorrect ~ Portugal (checked speech)\n",
    "- YDYE = The Democratic Yemen --> now Yemen\n",
    "- CSK = Czechoslovakia --> divided into Czechia (CZE), and Slovakia (SVK)\n",
    "- YUG = Yugoslavia --> divided into Bosnia and Herzegovina (BA), Croatia (HR), the former Yugoslav Republic of Macedonia (MK), Serbia and Montenegro (CS), Slovenia (SI)\n",
    "\n",
    "**Actions:**\n",
    "- DDR to DEU (Germany)\n",
    "- POR to PRT (Portugal)\n",
    "- YDYE to YEM (Yemen)\n",
    "\n",
    "Unsure what to do with EU, CSK, and YUG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "91ce45dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # alpha3 codes: in df_speech but not in df_code\n",
    "# boolean = df_speech['ISO-alpha3 Code'].isin(df_code['ISO-alpha3 Code'])\n",
    "# alpha3_wo_code = df_speech[~boolean]['ISO-alpha3 Code'].unique()\n",
    "\n",
    "alpha_replace = {'POR':'PRT', \n",
    "                 'YDYE':'YEM',\n",
    "                 'DDR':'DEU'}\n",
    "\n",
    "df_speech['ISO-alpha3 Code'].replace(alpha_replace, inplace=True) # update to pair with df_code\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "022962d9",
   "metadata": {},
   "source": [
    "df_merge and df_happiness merged on **Country or Area -- Country name**\n",
    "\n",
    "**What Country Names are found in df_happiness but NOT df_code?**\n",
    "- df_happiness doesn't incldue all countries\n",
    "- Some countries in df_happiness aren't part of UN \n",
    "\n",
    "**Actions:**\n",
    "- see replacement dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "051919b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # country names: in df_happiness but not in df_code\n",
    "# boolean = df_happiness['Country name'].isin(df_code['Country or Area'])\n",
    "# country_wo_happiness = df_happiness[~boolean]['Country name'].unique()\n",
    "\n",
    "country_replace = {'Bolivia':'Bolivia (Plurinational State of)',\n",
    "                   'Congo (Brazzaville)':'Congo',\n",
    "                   'Congo (Kinshasa)':'Democratic Republic of the Congo',\n",
    "                   'Czech Republic':'Czechia',\n",
    "                   'Hong Kong S.A.R. of China':'China - Hong Kong Special Administrative Region',\n",
    "                   'Iran':'Iran (Islamic Republic of)',\n",
    "                   'Ivory Coast':'Côte d’Ivoire',\n",
    "                   'Laos':\"Lao People's Democratic Republic\",\n",
    "                   'Moldova':'Republic of Moldova',\n",
    "                   'North Cyprus':'Cyprus',\n",
    "                   'Palestinian Territories':'State of Palestine',\n",
    "                   'Russia':'Russian Federation',\n",
    "                   'Somaliland region':'Somalia',\n",
    "                   'South Korea':'Republic of Korea',\n",
    "                   'Swaziland':'Eswatini',\n",
    "                   'Syria':'Syrian Arab Republic',\n",
    "                   'Tanzania':'United Republic of Tanzania',\n",
    "                   'United Kingdom':'United Kingdom of Great Britain and Northern Ireland',\n",
    "                   'United States':'United States of America',\n",
    "                   'Venezuela':'Venezuela (Bolivarian Republic of)',\n",
    "                   'Vietnam':'Viet Nam'}\n",
    "\n",
    "df_happiness.replace(country_replace, inplace=True) # update to pair with df_code\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c133f28d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge three datasets\n",
    "df_merge = pd.merge(df_code, df_speech, \n",
    "                    how='right', on='ISO-alpha3 Code')\n",
    "\n",
    "df = pd.merge(df_merge, df_happiness,\n",
    "              how='left',\n",
    "              left_on=['Country or Area','Year'],\n",
    "              right_on=['Country name','Year']).drop('Country name', axis=1) # drop Country name (redundant)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d30776b",
   "metadata": {},
   "source": [
    "# Process Speeches"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "34432ea5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1min 48s, sys: 1.66 s, total: 1min 50s\n",
      "Wall time: 1min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk import word_tokenize\n",
    "import re\n",
    "\n",
    "def preprocess(s):\n",
    "    '''\n",
    "    input: s, speech string\n",
    "    output: tokenized list (wo stop words)\n",
    "    '''\n",
    "    sw = stopwords.words(\"english\") # stop words\n",
    "    \n",
    "    s = s.lower() # normalize lower case\n",
    "    s = re.sub('united nations','united_nations',s) # combine to avoid tokenization splitting\n",
    "    s = re.sub('\\s+',' ',s) # remove tabs and new lines\n",
    "    s = re.sub(r'[^a-zA-Z_ ]', '', s) # remove punctuation/non-alpha\n",
    "    s = re.sub(r'  +',' ', s) # remove 2+ spaces\n",
    "    s = s.strip() # remove leading-trailing spaces\n",
    "    \n",
    "    words = word_tokenize(s) # tokenize processed string\n",
    "    words = [word for word in words if (word not in sw)] # remove stop words\n",
    "\n",
    "    return words\n",
    "\n",
    "df['speech_processed'] = df['Speech'].apply(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e225789b",
   "metadata": {},
   "source": [
    "**Stemming**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "7256c8aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 3s, sys: 2.03 s, total: 3min 5s\n",
      "Wall time: 3min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "pstemmer = PorterStemmer()\n",
    "find_stem = lambda words: [pstemmer.stem(word) for word in words]\n",
    "df['speech_stem'] = df['speech_processed'].apply(find_stem)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79f43da3",
   "metadata": {},
   "source": [
    "**Lemmatization**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9cd414e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3min 3s, sys: 2.01 s, total: 3min 5s\n",
      "Wall time: 3min 7s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "wnl = WordNetLemmatizer()\n",
    "find_lemma = lambda words: [wnl.lemmatize(word) for word in words]\n",
    "df['speech_lemma'] = df['speech_processed'].apply(find_stem)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19441c0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
